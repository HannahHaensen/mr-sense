<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <title>MR-Sense: A Mixed Reality Environment Search Assistant for Blind and Visually Impaired People</title>
    <meta content="Mixed Reality for blind and visually impaired people" name="description"/>
    <meta content="MR-Sense: A Mixed Reality Environment Search Assistant for Blind and Visually Impaired People"
          property="og:title"/>
    <meta content="AI for blind and visually impaired people" name="description"/>
    <meta content="Computer Vision for blind and visually impaired people"
          name="description"/>
    <meta content="width=device-width, initial-scale=1" name="viewport"/>
    <style>
        a:link {
            text-decoration: none;
            color: darkred;
        }

        a:visited {
            text-decoration: none;
            color: darkred;
        }

        a:hover {
            text-decoration: none;
            color: darkred;
        }

        a:active {
            text-decoration: none;
            color: darkred;
        }
    </style>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
</head>
<body>
<div class="section">
    <br>
    <br>
    <div class="container">
        <div class="text-center">
            <h1 class="nerf_title_v2">MR-Sense: A Mixed Reality Environment Search Assistant for Blind and Visually
                Impaired
                People</h1>
            <h5 class="">Object detection meets Extended Reality. We present MR-Sense a mixed reality guidance tool
                utilizing
                deep learning for blind and visually impaired people.</h5>
            <br>

            <img src="images/aixvr.png" style="width: 100px;">
            <h5 class="">IEEE AIxVR, Hollywood 2024</h5>
            <br>
            <br>
            <div class="row">
                <div class="col-md-12">
                    <a href="https://hannahhaensen.github.io" target="_blank" class="authors_a">
                        Hannah Schieber,
                        <sup>1,2</sup></a>
                    <a href="https://scholar.google.com/citations?user=XnnWzYgAAAAJ&hl=de&oi=ao" target="_blank" class="authors_a">
                        Constantin Kleinbeck,
                        <sup>1,2</sup>
                    </a>
                    <a href="https://scholar.google.com/citations?user=cvZEdbwAAAAJ&hl=de&oi=ao" target="_blank" class="authors_a">
                        Luisa Theelke,
                        <sup>1,2</sup>
                    </a>

                        Miriam Kraft,
                        <sup>2</sup>

                    <a href="https://scholar.google.com/citations?user=hNhoihUAAAAJ&hl=de&oi=ao" target="_blank" class="authors_a">
                        Julian Kreimeier,
                        <sup>1</sup>
                    </a>
                    <a href="https://scholar.google.com/citations?user=YonEz-cAAAAJ&hl=de&oi=ao" target="_blank" class="authors_a">
                        Daniel Roth<sup>1</sup>
                    </a>
                </div>
            </div>
            <div class="row">
                <div class="col-md-6">

                    <div class="nerf_mobile_inst">
                        <span class="text-span_nerf">1 </span>Technical University of Munich, School of Medicine
                        and
                        Health, Department Clinical Medicine, Klinikum rechts der Isar, Orthopedics and Sports
                        Orthopedics, Munich, Germany
                    </div>


                </div>
                <div class="col-md-6">

                    <div class="nerf_mobile_inst">
                        <span class="text-span_nerf">2 </span>Human-Centered Computing and Extended Reality,
                        Friedrich-Alexander-Universität Erlangen-Nürnberg,
                        Erlangen, Germany
                    </div>

                </div>


            </div>

            <br>
            <br>

            <div class="row">
                <div class="col-md-6">
                    <a href="https://github.com/HannahHaensen/mr-sense">
                        <img src="images/github.png" style="width: 100px;">
                    </a>
                    <p>Coming soon</p>
                </div>
                <div class="col-md-6">
                    <a href="/docs/pdf/2024_AIxVR_MR_Sense_preprint.pdf">
                        <img src="images/paper.png" style="width: 100px; border: black 1px solid;">
                    </a>
                    <p>Coming soon</p>
                </div>
            </div>
            <br>
            <br>
            <div class="row">
                <img src="images/teaser2.png">
            </div>
            <br>
            <br>
        </div>

        <div class="row">
            <div class="col-sm-12">
                <h5>
                    Abstract
                </h5>
                <p style="text-align: left">
                    Search tasks can be challenging for blind or visually impaired people. To determine an object's
                    location and to navigate there, they often rely on the limited sensory capabilities of a white cane,
                    search haptically, or ask for help. We introduce MR-Sense, a mixed reality assistant to support
                    search and navigation tasks. The system is designed in a participatory fashion and utilizes sensory
                    data of a standalone mixed reality head-mounted display to perform deep learning-driven object
                    recognition and environment mapping. The user is supported in object search tasks via spatially
                    mapped audio and vibrotactile feedback. We conducted a preliminary user study including ten blind or
                    visually impaired participants and a final user evaluation with thirteen blind or visually impaired
                    participants. The final study reveals that MR-Sense alone cannot replace the cane but provides a
                    valuable addition in terms of usability and task load. We further propose a standardized evaluation
                    setup for replicable studies and highlight relevant potentials and challenges fostering future work
                    towards employing technology in accessibility.
                </p>
            </div>
        </div>
        <br>
        <br>
        <div class="row">
            <div class="col-lg-12 text-center">
                <img src="images/system.png" style="width: 80%;">
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-md-6">

                <div class="row">
                    <h5>System</h5>
                    <p style="text-align: left;">
                        MR-Sense is built using Unity, Python and Swift and runs on a HMD (HoloLens 2),
                        a smartwatch (Apple Watch Series 6), and an external server. The external server executes YOLOv5
                        to detect objects in the image stream of the Hololens and the Apple Watch is used to provide
                        vibrotactile feedback.</p>
                </div>
                <div class="row">
                    <img src="images/system_fig_with_server.png" style="width: 100%;">
                </div>
            </div>
            <div class="col-md-6">
                <div class="row">
                    <h5>Reproducibly Study Apparatus</h5>
                    <p style="text-align: left;">
                        As a realistic and reproducible experimental setup, we prepared a common indoor room with two
                        shelves, a table and a chair.
                        All items were purchased at IKEA </p>
                    <ul style="text-align: left;">
                        <li>KALLAX, white, 77x39x77 cm</li>
                        <li>KALLAX, white, 42x39x112 cm</li>
                        <li>LINNMON / ADILS Desk darkgrey/black, 100x60x73 cm</li>
                        <li>GUNDE Chair, black</li>
                    </ul>
                    <p style="text-align: left;">
                        As search objects, we selected a banana, a laptop, a bottle and a flower (these four objects are
                        within the 80 categories of the COCO dataset mentioned above).
                    </p>
                </div>
                <div class="row">
                    <img src="images/room.png" style="width: 100%;">
                </div>
            </div>
        </div>
        <br>
        <br>
        <div class="row">
            <div class="col-md-12">
                <h5 class="grey-heading_nerf">
                    Citation</h5>
                <p class="" style="text-align: left; background: aliceblue; margin: 20px; padding: 20px;">
                    TBA
                </p>
            </div>
        </div>
        <br>
        <br>
        <div class="row">
            <p>We gratefully acknowledge funding for this study by d.hip campus</p>
        </div>
        <div class="row">
            <div class="col-6">
                        <img src="images/tum.png" style="width: 30%"><br>
            </div>
            <div class="col-6">
                        <img src="images/fau.jpg" style="width: 30%"><br>
            </div>
        </div>
        <br>
        <br>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4"
            crossorigin="anonymous"></script>

</body>
</html>
